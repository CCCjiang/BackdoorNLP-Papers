# BackdoorNLP-Papers



  ![](https://img.shields.io/github/last-commit/CCCjiang/BackdoorNLP-Papers?color=blue) ![](https://img.shields.io/badge/PaperNumber-5-brightgreen) ![](https://img.shields.io/badge/PRs-Welcome-red) 

### Contents 
* [1. Attack Papers](#2-attack-papers)
* [2. Defense Papers](#2-defense-papers)
* [3. Natural Backdoor Papers](#3-natural-backdoor-papers)



## 1. Attack Papers
1. **Weight Poisoning Attacks on Pre-trained Models**. *Keita Kurita , Paul Michel, Graham Neubig*. ACL 2020. [[pdf](https://arxiv.org/abs/2004.06660)] [[code](https://github.com/neulab/RIPPLe)]
2. **BadNL: Backdoor Attacks Against NLP Models**. *Xiaoyi Chen, Ahmed Salem, Michael Backes, Shiqing Ma, Yang Zhang*. Preprint. [[pdf](https://arxiv.org/abs/2006.01043)]
3. **Trojaning Language Models for Fun and Profit**. *Xinyang Zhang, Zheng Zhang, Ting Wang*. Preprint. [[pdf](https://arxiv.org/abs/2008.00312v1)]

## 2. Defense Papers
No


## 3. Natural Backdoor Papers
1. **Universal Adversarial Triggers for Attacking and Analyzing NLP**. *Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, Sameer Singh*. EMNLP 2019. [[pdf](https://arxiv.org/abs/1908.07125)] [[code](https://github.com/Eric-Wallace/universal-triggers)]
1. **Universal Adversarial Attacks with Natural Triggers for Text Classification**. *Liwei Song, Xinwei Yu, Hsuan-Tung Peng, Karthik Narasimhan*. Preprint. [[pdf](https://arxiv.org/abs/2005.00174)] [[code]( https://github.com/Hsuan-Tung/universal_attack_natural_trigger)]


